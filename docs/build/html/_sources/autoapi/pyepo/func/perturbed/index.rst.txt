pyepo.func.perturbed
====================

.. py:module:: pyepo.func.perturbed

.. autoapi-nested-parse::

   Perturbed optimization function



Classes
-------

.. autoapisummary::

   pyepo.func.perturbed.perturbedOpt
   pyepo.func.perturbed.perturbedOptFunc
   pyepo.func.perturbed.perturbedFenchelYoung
   pyepo.func.perturbed.perturbedFenchelYoungFunc
   pyepo.func.perturbed.implicitMLE
   pyepo.func.perturbed.implicitMLEFunc
   pyepo.func.perturbed.adaptiveImplicitMLE
   pyepo.func.perturbed.adaptiveImplicitMLEFunc


Functions
---------

.. autoapisummary::

   pyepo.func.perturbed._solve_or_cache
   pyepo.func.perturbed._solve_in_pass
   pyepo.func.perturbed._cache_in_pass


Module Contents
---------------

.. py:class:: perturbedOpt(optmodel, n_samples=10, sigma=1.0, processes=1, seed=135, solve_ratio=1, dataset=None)

   Bases: :py:obj:`pyepo.func.abcmodule.optModule`


   An autograd module for Fenchel-Young loss using perturbation techniques. The
   use of the loss improves the algorithm by the specific expression of the
   gradients of the loss.

   For the perturbed optimizer, the cost vector needs to be predicted from
   contextual data and is perturbed with Gaussian noise.

   Thus, it allows us to design an algorithm based on stochastic gradient
   descent.

   Reference: <https://papers.nips.cc/paper/2020/hash/6bb56208f672af0dd65451f869fedfd9-Abstract.html>


   .. py:attribute:: n_samples
      :value: 10



   .. py:attribute:: sigma
      :value: 1.0



   .. py:attribute:: rnd


   .. py:method:: forward(pred_cost)

      Forward pass



.. py:class:: perturbedOptFunc(*args, **kwargs)

   Bases: :py:obj:`torch.autograd.Function`


   An autograd function for perturbed optimizer


   .. py:method:: forward(ctx, pred_cost, module)
      :staticmethod:


      Forward pass for perturbed

      :param pred_cost: a batch of predicted values of the cost
      :type pred_cost: torch.tensor
      :param module: perturbedOpt module
      :type module: optModule

      :returns: solution expectations with perturbation
      :rtype: torch.tensor



   .. py:method:: backward(ctx, grad_output)
      :staticmethod:


      Backward pass for perturbed



.. py:class:: perturbedFenchelYoung(optmodel, n_samples=10, sigma=1.0, processes=1, seed=135, solve_ratio=1, reduction='mean', dataset=None)

   Bases: :py:obj:`pyepo.func.abcmodule.optModule`


   An autograd module for Fenchel-Young loss using perturbation techniques. The
   use of the loss improves the algorithm by the specific expression of the
   gradients of the loss.

   For the perturbed optimizer, the cost vector needs to be predicted from
   contextual data and is perturbed with Gaussian noise.

   The Fenchel-Young loss allows directly optimizing a loss between the features
   and solutions with less computation. Thus, it allows us to design an algorithm
   based on stochastic gradient descent.

   Reference: <https://papers.nips.cc/paper/2020/hash/6bb56208f672af0dd65451f869fedfd9-Abstract.html>


   .. py:attribute:: n_samples
      :value: 10



   .. py:attribute:: sigma
      :value: 1.0



   .. py:attribute:: rnd


   .. py:method:: forward(pred_cost, true_sol)

      Forward pass



.. py:class:: perturbedFenchelYoungFunc(*args, **kwargs)

   Bases: :py:obj:`torch.autograd.Function`


   An autograd function for Fenchel-Young loss using perturbation techniques.


   .. py:method:: forward(ctx, pred_cost, true_sol, module)
      :staticmethod:


      Forward pass for perturbed Fenchel-Young loss

      :param pred_cost: a batch of predicted values of the cost
      :type pred_cost: torch.tensor
      :param true_sol: a batch of true optimal solutions
      :type true_sol: torch.tensor
      :param module: perturbedFenchelYoung module
      :type module: optModule

      :returns: solution expectations with perturbation
      :rtype: torch.tensor



   .. py:method:: backward(ctx, grad_output)
      :staticmethod:


      Backward pass for perturbed Fenchel-Young loss



.. py:class:: implicitMLE(optmodel, n_samples=10, sigma=1.0, lambd=10, distribution=None, two_sides=False, processes=1, solve_ratio=1, dataset=None)

   Bases: :py:obj:`pyepo.func.abcmodule.optModule`


   An autograd module for Implicit Maximum Likelihood Estimator, which yields
   an optimal solution in a constrained exponential family distribution via
   Perturb-and-MAP.

   For I-MLE, it works as black-box combinatorial solvers, in which constraints
   are known and fixed, but the cost vector needs to be predicted from
   contextual data.

   The I-MLE approximates the gradient of the optimizer smoothly. Thus, it allows us to
   design an algorithm based on stochastic gradient descent.

   Reference: <https://proceedings.neurips.cc/paper_files/paper/2021/hash/7a430339c10c642c4b2251756fd1b484-Abstract.html>


   .. py:attribute:: n_samples
      :value: 10



   .. py:attribute:: sigma
      :value: 1.0



   .. py:attribute:: lambd
      :value: 10



   .. py:attribute:: distribution
      :value: None



   .. py:attribute:: two_sides
      :value: False



   .. py:method:: forward(pred_cost)

      Forward pass



.. py:class:: implicitMLEFunc(*args, **kwargs)

   Bases: :py:obj:`torch.autograd.Function`


   An autograd function for Implicit Maximum Likelihood Estimator


   .. py:method:: forward(ctx, pred_cost, module)
      :staticmethod:


      Forward pass for IMLE

      :param pred_cost: a batch of predicted values of the cost
      :type pred_cost: torch.tensor
      :param module: implicitMLE module
      :type module: optModule

      :returns: predicted solutions
      :rtype: torch.tensor



   .. py:method:: backward(ctx, grad_output)
      :staticmethod:


      Backward pass for IMLE



.. py:class:: adaptiveImplicitMLE(optmodel, n_samples=10, sigma=1.0, distribution=None, two_sides=False, processes=1, solve_ratio=1, dataset=None)

   Bases: :py:obj:`pyepo.func.abcmodule.optModule`


   An autograd module for Adaptive Implicit Maximum Likelihood Estimator, which
   adaptively chooses hyperparameter λ and yields an optimal solution in a
   constrained exponential family distribution via Perturb-and-MAP.

   For AI-MLE, it works as black-box combinatorial solvers, in which constraints
   are known and fixed, but the cost vector needs to be predicted from
   contextual data.

   The AI-MLE approximates the gradient of the optimizer smoothly. Thus, it allows us to
   design an algorithm based on stochastic gradient descent.

   Reference: <https://ojs.aaai.org/index.php/AAAI/article/view/26103>


   .. py:attribute:: n_samples
      :value: 10



   .. py:attribute:: sigma
      :value: 1.0



   .. py:attribute:: distribution
      :value: None



   .. py:attribute:: two_sides
      :value: False



   .. py:attribute:: alpha
      :value: 0



   .. py:attribute:: grad_norm_avg
      :value: 1



   .. py:attribute:: step
      :value: 0.001



   .. py:method:: forward(pred_cost)

      Forward pass



.. py:class:: adaptiveImplicitMLEFunc(*args, **kwargs)

   Bases: :py:obj:`implicitMLEFunc`


   An autograd function for Adaptive Implicit Maximum Likelihood Estimator


   .. py:method:: backward(ctx, grad_output)
      :staticmethod:


      Backward pass for IMLE



.. py:function:: _solve_or_cache(ptb_c, module)

   Solve or use cached solutions for perturbed costs (3D: n_samples × batch × vars).
   Delegates to the shared 2D functions in utils after flattening.


.. py:function:: _solve_in_pass(ptb_c, optmodel, processes, pool, solpool=None, solset=None)

   Solve optimization for perturbed 3D costs and update solution pool.

   :param ptb_c: perturbed costs, shape (n_samples, batch, vars)
   :type ptb_c: torch.tensor
   :param optmodel: optimization model
   :type optmodel: optModel
   :param processes: number of processors
   :type processes: int
   :param pool: process pool
   :param solpool: solution pool
   :type solpool: torch.tensor
   :param solset: hash set for deduplication
   :type solset: set

   :returns: (solutions shape (batch, n_samples, vars), updated solpool)
   :rtype: tuple


.. py:function:: _cache_in_pass(ptb_c, optmodel, solpool)

   Use solution pool for perturbed 3D costs (n_samples × batch × vars).
   Unlike the 2D version in utils, this handles the extra sample dimension.


